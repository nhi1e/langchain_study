{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7ce2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "if \"LANGCHAIN_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangChain API key: \")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145500f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 document with length 43047\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# 1: Load: load data with document loaders (using WebBaseLoader which uses BeutifulSoup)\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# only get relevant HTML elements\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer}, # only load specific HTML elements\n",
    ")\n",
    "\n",
    "# load the data\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"loaded\", len(docs), \"document with length\", len(docs[0].page_content))  # check the number of documents and number of characters in the first document\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9120960a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split into 63 chunks with length 969\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 34990}\n"
     ]
    }
   ],
   "source": [
    "# 2: Split: split documents into smaller chunks \n",
    "#           smaller chunks can fit into context window and easier to search over and pass to LLM\n",
    "#            also for embedding and vector store\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # size of each chunk\n",
    "    chunk_overlap=200,      # overlap between chunks (to preserve context in case some information is split across chunks)\n",
    "    add_start_index=True,   # store start index to each chunk\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(\"split into\", len(all_splits), \"chunks with length\", len(all_splits[0].page_content))  # check the number of chunks and number of characters in the first chunk\n",
    "print(all_splits[50].metadata)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c6ac0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Embed: embed our chunks and store them in a vector store\n",
    "\n",
    "from langchain_chroma import Chroma             # vector store\n",
    "from langchain_openai import OpenAIEmbeddings   # embedding model\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=all_splits, embedding = OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88534ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 5 documents\n",
      "Document 1:\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an \n",
      "Metadata: {'start_index': 1638, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "Document 2:\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an \n",
      "Metadata: {'start_index': 1638, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "Document 3:\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe\n",
      "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "\n",
      "Document 4:\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe\n",
      "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "\n",
      "Document 5:\n",
      "Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and effici\n",
      "Metadata: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29546}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4: Retrieve: \n",
    "\n",
    "# turn vector store into a retriever\n",
    "# use similarity search to find the most relevant chunks\n",
    "# return the top 5 most relevant chunks\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}) \n",
    "\n",
    "# retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "\n",
    "# print(\"retrieved\", len(retrieved_docs), \"documents\")\n",
    "# for i, doc in enumerate(retrieved_docs):\n",
    "#     print(f\"Document {i+1}:\")\n",
    "#     print(doc.page_content[:500])  # print first 500 characters of each document\n",
    "#     print(\"Metadata:\", doc.metadata)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ef89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a good question! However, I don't know the answer to that. Thank you for the question!\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5: Generate: generate a response using the LLM and the retrieved documents\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") # load prompt from LangChain Hub\n",
    "\n",
    "# example_query = prompt.invoke(\n",
    "#     {\"context\": \"filler context\",\n",
    "#      \"question\": \"filler question\"}\n",
    "# ).to_messages()\n",
    "\n",
    "# print(\"Example query:\", example_query[0].content)  # print the example query\n",
    "\n",
    "# chain that takes a question, retrieve documents, construct prompt, passes to LLM, and returns response\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "    You are a helpful assistant for question-answering tasks.\n",
    "    Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Always validate it's a good question at the beginning of your answer. \n",
    "    Always say thank you for the question at the end of your answer.\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\n",
    "\"\"\"\n",
    "custom_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser   # output parser to convert LLM response to string\n",
    "from langchain_core.runnables import RunnablePassthrough    # passthrough to pass the question through\\\n",
    "\n",
    "def formated_docs(docs):                    #take all the retrieved documents and format them into a string\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#chaining\n",
    "rag_chain = ( #feed direction: L->R\n",
    "    {\"context\" : retriever | formated_docs, # take retrieved documents and format them into string that can feed into LLM\n",
    "     \"question\" : RunnablePassthrough(),    # forwards the query to the LLM\n",
    "    }\n",
    "    | custom_prompt                         # content for {context} and {question} are injected into the prompt \n",
    "    | llm                                   # the prompt is passed to the LLM      \n",
    "    | StrOutputParser()                     # parse the LLM output to string\n",
    ")\n",
    "\n",
    "    \n",
    "rag_chain.invoke(\"What is the name of my son\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4b3c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
